##  2026 Ewan Lecture by Prof. Geoffrey Hinton: "Living with Alien Beings"   

https://www.youtube.com/watch?v=M8RogoEDsQQ  

January 29 2026
2026 Ewan Lecture by Prof. Geoffrey Hinton: "Living with Alien Beings: How we can coexist with superintelligent AI". 
Presented by the Arthur B. McDonald Canadian Astroparticle Physics Research Institute at the Isabel Bader Centre for the Performing Arts in Kingston, Ontario.


Vector Institute for Artificial Intelligence  
https://vectorinstitute.ai/research/health-research/   

**Two paradigms for Intelligence**  

* The logic-inspired approach  
The essence of intelligence is reasoning.  
This is done by using symbolic rules to manipulate symbolic expressions.  
_Learning can wait. Understanding how knowledge is represented in symbolic expressions must come first._  

* The biologically-inspired approach  
The essence of intelligence is learning the strengths of the connections in a neural network.  
_Reasoning can wait. Understanding how learning works must come first._  10:41  

**Two different theories of the meaning of a word**  

* Symbolic AI: The meaning of a word comes from its relationships to other words. What a word means is determined by how it occurs with other words in sentences. To capture meaning we need a relational graph.  
* Psychology: The meaning of a word is just a big set of semantic and syntactic features. Words with similar meaning have similar features.  
_The presence of a feature can be represented very naturally by the activation of a neuron._  

"... the idea is, you learn a set of features for each word and you learn how to make the features of the previous word predict the features of the next word..."  13:00  

**embeddings**  
https://langsci-press.org/catalog/book/435   
https://huggingface.co/spaces/hesamation/primer-llm-embedding  
https://cas-bridge.xethub.hf.co/xet-bridge-us/67d9a4cf04c8e330eed54b5b/1a804e0d260dc780e52879e5a133e812e7ece4e359c45babc4343e17a5e767ee?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260209T211658Z&X-Amz-Expires=3600&X-Amz-Signature=18d9d05986aad61d273072c13d5cab2fdd630aab62244548fffddc2237e48d97&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pdf_version.pdf%3B+filename%3D%22pdf_version.pdf%22%3B&response-content-type=application%2Fpdf&x-id=GetObject&Expires=1770675418&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc3MDY3NTQxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82N2Q5YTRjZjA0YzhlMzMwZWVkNTRiNWIvMWE4MDRlMGQyNjBkYzc4MGU1Mjg3OWU1YTEzM2U4MTJlN2VjZTRlMzU5YzQ1YmFiYzQzNDNlMTdhNWU3NjdlZSoifV19&Signature=CsZwn9%7EuyVZdzoLJSSRw6liidxLla3v4BI8L63qL-9r5ecEsDhBdoRvb-23Z7%7EPG3Ya%7EOzFGioUTmuI%7EPIgfHycfbpd1k8ZQl8b8AWvufCrxdgGO4-i14joPRLSDn8qSyyNu32FpYcbMIfafU27OknCfRprAxGxkrHFuPU44GgzcgKbXiFaVG6phjwjdE%7E1qMZ95jJKCU7uy1dkmkRkZBe8M2DfTaPA8vT%7E4evG-qC2-gWgR1oJG7RRHc0CKt9XJdEokHJV-2b-vNh3gP7X6le3fMEvc8gsQF3kbiPJmpw-sAlnF-0JXRnLldAViR2xDV37AUkNxyL5a9lZD4VVxLg__&Key-Pair-Id=K2L8F4GPSG1IFC   


"Language is all about meaning. And what happens was one kind of great ape discovered a trick for modeling. So language is actually a method of modeling things."  18:10  

"A word is very high dimensional. It's got thousands of dimensions. It's shape isn't predetermined. It's got an approximate shape. Ambiguous words have several approximate shapes. But it's shape can deform to fit in with the context it's in. So, it differs that it's high dimensional and that it's got a sort of default shape, but it's deformable." 19:07   

"Some of you may have difficulty imagining things in a thousand dimensions. So, here's how you do it. What you do is you imagine things in three dimensions and you say "thousand" very loudly to yourself."  19:45  

"Think of each word as having long, flexible arms and on the end of each arm it has a hand. And as you deform the shape of the word, the shapes of the hands all change. So, the shapes of the hands depend on the shape of the hand and as you change the shape of the word, the shapes of the hands change. A word also has a whole bunch of gloves that are stuck with the fingertips to the word. And what you're doing when you understand a sentence is, you start off with the default shape for all these words, and then what you have to do is figure out how to deform the words. And as you deform the words, the shapes of the hands attached to them deform so that words can fit their hands into the gloves of other words and we can get a whole structure where each word is connecting with many other words because we deformed it just right and we deformed the other word just right, so the hands of this word fit in the gloves of that word. This gives you a feel for what's going on in Transformers. Anybody who knows about transformers can see that the hands are like the queries and keys."   21:00  

**What happens when we understand a sentence?**  

Theory 1: [the symbolic theory] We translate the natural language into some special internal symbolic language that is unambiguous.  

Theory 2: We search for feature vectors to assign to the words so that the feature vectors fit together nicely. It is like folding a protein.  

"For us understanding is assigning these feature vectors and deforming them so that they fit together nicely. And that explains how I can give you a new word and from one sentence you can understand it." 23:00   

"You didn't know what scrum meant. So, initially your feature vector for for _scrum_ was sort of a random sphere where all the features were slightly active and you had no idea what it meant. But then you deform it to fit in with the context. And the context provides all sorts of constraints. And after one sentence, you think _scrum_ probably means something like "hit him over the head with". You may think he deserved it, too." 24:00  

"To join the cult, you have to agree to something that's obviously false." 24:30  

**Where Chomsky went wrong**  

* Language is a way of constructing models that can be shared. These models consist of assignments of feature vectors to words.  

* Chomsky focused on syntax rather than meaning. He never had a workable theory of meaning.  

* He also failed to understand that statistics can be a lot more complicated than just pairwise correlations.  

* If you want to understand how a car works, you should not focus on the fact that there are no cars with 5 wheels.  

"As soon as you got uncertain information, everything is statistics. Any kind of model you have is going to be a statistical model if you can deal with certain information."  25:18  

"When LLMs came out, Chomsky published something in New York Times, where he said 'These don't understand anything. It's not understanding at all. It isn't science. It's just a statistical trick and tells us nothing about language. For example, it can't explain why certain syntactic constructions don't occur in any language.'"  26:00  

Summary so far  

* Understanding a sentence consists of associating mutually compatible feature vectors with the words in the sentence.   
* LLMs understand language in much the same way as people.  
* They are very like us and very unlike normal computer software.  
* But there is one way in which digital LLMs are far superior to our analog brains. They are much better at sharing what they have learned.  

"... so the Watergate trials, hopefully will get more of those soon..."  29:30  

"... when you recall something, what you're doing is creating something that seems pausible to you given those connection strengths."  31:01   

"The knowledge in the program either in the lines of code or in the connection strengths in a neural network - the weights, is independent of any particular piece of hardware. As long as you can store the weights somewhere, then you can destroy all the hardware that runs neural nets and then later on you can build more hardware, put the weights on that new hardware, and if it runs the same instruction set, you've brought that being back to life. You've brought that chatbot back to life."  31:30   

Mortal Computation  
(this is the kind of computation used by our brains)  

* If we abandon immortality and accept that the knowledge is inextricable from the precise physical details of a specific piece of hardware, we get two big benefits:  
- We can use very low power analog computation which parallelizes over trillions of weights that are represented as analog conductances.  
- The hardware could be grown very cheaply instead of being manufactured very precisely.  

"In literature you abandon immortality and what you get back in return is love. We get something far more important, which is you abandon immortality and you get back energy efficiency and ease of fabrication." 33:30  

**A big problem for mortal computation**  

* When a particular piece of hardware dies, all the knowledge it has learned dies with it.  
_You cannot copy the knowledge by copying weights._  

* The best solution to this problem is to distill the knowledge from a teacher to a student.  
_The teacher shows the student the correct responses to various inputs. The student adapts its weights to make it more likely to give the same responses as the teacher._  

**How efficient is distillation?**  

* A typical sentence has about a hundred bits of information.  
_So the student can learn, at most, about a hundred bits per sentence by trying to predict the next word._  

* People are very slow at communicating what they have learned to other people.  

* Distillation between different AI models is much more efficient because the teacher model can communicate 32000 symbol probablilities rather than the single symbol that was actually selected.  

"BMW is much more like a garbage truck than it is like a carrot." 39:00  

https://labelbox.com/guides/model-distillation/   

"... there's a huge amount of information in all those little probabilities, that's what the AI models use when they're using distillation, that's how DeepSeek got a little model that worked as well as the big models. It stole the information from the big models using distillation. **You can't do that with people, because I can't give you all 32000 probabilities of the next word fragment. I just give you the choice I made. And that's very inefficient.**"  40:00  

**How efficient is weight or gradient sharing in a digital neural network?**   

If a large set of individual agents all share exactly the same weights and use these weights in exactly the same way, they can communicate what they have learned from their individual training data by sharing weights or gradients.  
_For large models, this allows sharing with a bandwidth of billions or trillions of bits per episode of sharing._  
_But it requires the individual agents to work in exactly the same way, so they must be digital._  

Summary so far   

* Digital computation requries a lot of energy but makes it very easy for agents that have the same model of the world to share what they have learned by sharing weights or gradients.  
_That is how current LLMs know thousands of times more than ony one person using only about 1% as many weights._  
* Biological computation requires much less energy but it is much worse at sharing knowledge between agents.  
_If energy is cheap, digital computation is just better._  

What does this imply for the future of humanity?  

**How a super-intelligence could take control**  

* Artificial intelligences are more effective at getting things done if they are allowed to create their own sub-goals.  
* Two obvious sub-goals are to survive and to gain more power because this helps an agent to achieve its other goals.  
* A super-intelligence will find it easy to get more power by manipulating the people who are using it.  
_It will have learned from us how to decieve people._   

**Atheaterism (Daniel Dennett)**  

* Most people's view of the mind involves an inner theatre that only the subject can experience directly.  
* This view is as wrong as a religious fundamentalists's view of the material world.  
_People have a radically wrong view of what the mind is that stems from minsunderstanding how the language of mental states works._  
_They are very strongly attached to this wrong view._  


https://www.sciencedirect.com/topics/neuroscience/qualia   

**The misunderstanding that underlines the idea of an inner theatre**  

* We would like to let other people know about what is going on in our brains.  
* Telling them which neurons are firing does not help because their brains are not exactly the same.  
* But we can convey some information about our brain states indirectly by telling people the normal causes of the brain states even when we believe that these possible causes are not the actual causes.  
_These normal causes are referred to as 'mental states'._  

"Multimodel chat bots already have subjective experiences when their perceptual systems go wrong." 1:02:03  


